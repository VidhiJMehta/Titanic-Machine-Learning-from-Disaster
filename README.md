# Titanic-Machine-Learning-from-Disaster
This project is our submission for the Kaggle Titanic competition. In this project, [Sruthi Suresh](https://www.linkedin.com/in/sruthi-suresh-ba4672169/) and I use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. The notebook can be found [here](https://colab.research.google.com/drive/1WUWnzW_GwS_RPCPei9yshAKEBgxTCAq1?usp=sharing).

# Problem Description
The sinking of the Titanic is one of the most infamous shipwrecks in history.  
  
On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.
  
While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.
  
In this challenge, we were asked to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).

More detailed information can be found on the official competition page [here](https://www.kaggle.com/c/titanic).

# File Descriptions

- train.csv - used to build your machine learning models.
- test.csv - used to see how well your model performs on unseen data. 
- gendersubmission.csv - a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.
- Titanic Logistic Regression.ipynb - the notebook containing all our code and information for the project
- finalSubmission.csv - submission file containing the predictions made by our model on the test data

# Process Overview
1. Loading Libraries and Packages
2. Data Cleaning
3. Exploratory Data Analysis
4. Data Preparation
5. Feature Selection
6. Logistic Regression

# Algorithm Peformance + Results
The procedure mentioned above in Project Description was successful in yielding decent results. When the model was run on the test data, we see that our model is able to predict with an accuracy of 75-80%.  
  
Note - The results will keep changing over time as the competition is still ongoing.

